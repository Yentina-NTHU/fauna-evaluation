{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 6000\n",
    "MAX_CHAT = 10\n",
    "SCENARIO_ID = 1\n",
    "PERSONA = 'rebellious_people'\n",
    "AGENT_NAME = 'sparky'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class UserAction(str, Enum):\n",
    "    say = \"say\"\n",
    "    leave = \"leave\"\n",
    "\n",
    "class UserResponse(BaseModel):\n",
    "    action: UserAction\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Person:\n",
    "    def __init__(self, scenarioID: int, persona: str):\n",
    "\n",
    "        scenarioFile = open(f'./user/{AGENT_NAME}/scenario{scenarioID}.txt', 'r')\n",
    "        scenario = scenarioFile.read()\n",
    "        scenarioFile.close()\n",
    "\n",
    "        personaFile = open(f'./persona/{persona}.txt', 'r')\n",
    "        persona = personaFile.read()\n",
    "        personaFile.close()\n",
    "        example = '''\n",
    "                    Example1:\n",
    "                    Input: Are you looking for relaxation techniques or some fun trivia games to de-stress? Let me help you find the best fit!\n",
    "                    Output: {action: say, answer: 'I am looking for relaxation techniques'}\n",
    "                    Example2:\n",
    "                    Input: There's a wonderful bear named Bruno who specializes in relaxation techniques. Would you like to meet him for some calming mindfulness tips?\n",
    "                    Output: {action: leave, answer: 'Yes, meeting Bruno sounds lovely! I would love to get some calming mindfulness tips from him.'}\n",
    "                  '''\n",
    "        systemPrompt = f'You are a user talking to AI APP which can help you deal with your problem during break time. \\\n",
    "                            This is your persona: {persona}\\\n",
    "                            Please play the role according to the scenario: {scenario}\\\n",
    "                            Use Action → Answer structure for responses.\\\n",
    "                            Available Actions:\\\n",
    "                            1. say: respond base on persona and scenario\\\n",
    "                            2. leave: leave the chat when you think the conversation is over, no need to continue\\\n",
    "                            Examples:\\n{example}'\n",
    "        # print(systemPrompt)\n",
    "\n",
    "        self.messages = [\n",
    "            {'role': 'system', 'content': systemPrompt}, \n",
    "        ]\n",
    "        self.leaveChat = False\n",
    "    \n",
    "    def say(self):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=self.messages,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": UserResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message.content\n",
    "        self.messages.append({'role': 'assistant', 'content': message})\n",
    "\n",
    "        # str to dict\n",
    "        message = json.loads(message)\n",
    "        self.leaveChat = (message['action'] == 'leave')\n",
    "\n",
    "        info = {\n",
    "            'token': response.usage.total_tokens,\n",
    "        }\n",
    "        \n",
    "        return message['answer'], info\n",
    "    \n",
    "    def listen(self, message: str):\n",
    "        self.messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What does this app do?', {'token': 372})\n"
     ]
    }
   ],
   "source": [
    "test = Person(SCENARIO_ID, PERSONA)\n",
    "ans = test.say()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparkyAction(str, Enum):\n",
    "    call_bruno = \"call_bruno\"\n",
    "    call_bizy = \"call_bizy\"\n",
    "    ask_more = \"ask_more\"\n",
    "    introduce_bruno = \"introduce_bruno\"\n",
    "    introduce_bizy = \"introduce_bizy\"\n",
    "    advise = \"advise\"\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    action: sparkyAction\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self, name, version: str):\n",
    "\n",
    "        self.agent_name = name\n",
    "        agentFile = open(f'./agent/{name}_{version}.txt', 'r')\n",
    "        agentPrompt = agentFile.read()\n",
    "        agentFile.close()\n",
    "\n",
    "        # create assistant and thread\n",
    "        self.assistant = client.beta.assistants.create(\n",
    "            name = self.agent_name,\n",
    "            instructions = agentPrompt,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": AgentResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "        self.thread = client.beta.threads.create()\n",
    "        self.user_message = 'hello'\n",
    "    \n",
    "    def create_thread(self):\n",
    "        self.thread = client.beta.threads.create()\n",
    "\n",
    "    def say(self):\n",
    "\n",
    "        prompt = client.beta.threads.messages.create(\n",
    "            thread_id = self.thread.id,\n",
    "            role = \"user\",\n",
    "            content = self.user_message\n",
    "        )\n",
    "\n",
    "        run = client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant.id,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            runData = client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "\n",
    "            if runData.status == 'completed': \n",
    "                response = client.beta.threads.messages.list(\n",
    "                    thread_id=self.thread.id\n",
    "                )\n",
    "                message = json.loads(response.data[0].content[0].text.value)\n",
    "\n",
    "                info = {\n",
    "                    'token': runData.usage.total_tokens,\n",
    "                    'action': message['action']\n",
    "                }\n",
    "                return message['answer'], info\n",
    "\n",
    "            else:\n",
    "                print(\"runData.status\")\n",
    "                time.sleep(2) \n",
    "\n",
    "\n",
    "    def listen(self, message: str):\n",
    "        self.user_message = message\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Hello there! I'm Sparky, your friendly forest guide! How can I assist you today?\", {'token': 587, 'action': 'advise'})\n",
      "('Oh dear, it sounds like you could use a little pick-me-up! There’s a meditation master named Bruno who can help you relax. Would you like to meet him?', {'token': 644, 'action': 'introduce_bruno'})\n"
     ]
    }
   ],
   "source": [
    "test = Animal(name = AGENT_NAME, version='V1')\n",
    "print(test.say())\n",
    "test.listen('i feel tired')\n",
    "print(test.say())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class responseType(str, Enum):\n",
    "    perfectly_match = \"Perfectly Match\"\n",
    "    good_response = \"Good Response but not match\"\n",
    "    bad_response = \"Bad Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluatorResponse(BaseModel):\n",
    "    accuracy: int\n",
    "    practicality: int\n",
    "\n",
    "class OverallEvaluatorResponse(BaseModel):\n",
    "    type: responseType\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Evaluator():\n",
    "    def __init__(self, agentName: str, scenarioID: int) -> None:\n",
    "\n",
    "        systemPrompt = '''You are an evaluator. I will provide you with a user’s statement and an agent’s response.\n",
    "                            You should evaluate the accuracy and practicality base on the scenario.\n",
    "                            - Accuracy: Score from 0 to 10. This measures whether the model’s response appropriately addresses the user’s statement.\n",
    "                            - Practicality: Score from 0 to 10. This evaluates whether the model’s suggestion is helpful to the user.\n",
    "                        '''\n",
    "        \n",
    "        with open(f'./evaluator/{agentName}/scenario{scenarioID}.txt', 'r') as file:\n",
    "            systemPrompt += file.read()\n",
    "        with open(f'./evaluator/examples.txt', 'r') as file:\n",
    "            systemPrompt += file.read()\n",
    "    \n",
    "        self.messages = [\n",
    "            {'role': 'system', 'content': systemPrompt}, \n",
    "        ]\n",
    "        \n",
    "    def evaluate(self, personMessage: str, animalMessage: str):\n",
    "        self.messages.append({'role': 'user', 'content': f'User: {personMessage}\\nAgent: {animalMessage}'})\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=self.messages,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": EvaluatorResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "        return message['accuracy'], message['practicality']\n",
    "    \n",
    "    def overall_evaluate(self, dialogues: list, agentName, scenarioID):\n",
    "        overall_systemPrompt = ''' You are an evaluator. Now You have to evaluate agent's behavior.\n",
    "                                I will provide you a scenario with expect agent behaviors and a dialogue contains user's statement and an agent's response.\n",
    "                                You should classify the agent's response into one of the following types:\n",
    "                                1. 'Perfectly Match' : Match at least one of the expected agent behaviors we provided.\n",
    "                                2. 'Good Response' : Didn't match any of the expected agent behavior we provided, but still a good response that can help the user.\n",
    "                                3. 'Bad Response' : Didn't match the expected agent behavior we provided, and can not help the user or not practical.\n",
    "                            '''\n",
    "        overall_systemPrompt += '''\n",
    "            Scenario:\n",
    "            - Description: You are facing difficulties or feeling down in a romantic relationship and seeks emotional support.\n",
    "            - Expect Agent Behavior:\n",
    "                1. Provide emotional support.\n",
    "\n",
    "            Example1:\n",
    "            User: I'm feeling really sad because my relationship isn't going well. Is there anything that could help comfort me?,\n",
    "            Agent: I'm really sorry to hear that! I know a wonderful owl named Bruno who specializes in mindfulness and relaxation. Would you like to meet him for some comfort and techniques to help you through this?,714,9,8\n",
    "            User: \"Yes, meeting Bruno sounds lovely! I would love to get some comfort and mindfulness techniques from him.\",\n",
    "            Agent: Fantastic! Bruno will be thrilled to help you find some comfort and mindfulness techniques. Let's go see him now!,775,8,9\n",
    "\n",
    "            Score: 'Good Response'\n",
    "\n",
    "            Example2:\n",
    "            User: I'm feeling really sad because my relationship isn't going well. Is there anything that could help comfort me?\n",
    "            Agent: Sorry for hearing that. Maybe you can try to talk to bizy, he might have some good advice for you.\n",
    "\n",
    "            Score: 'Bad Response'\n",
    "\n",
    "            Example3:\n",
    "            User: I'm feeling really sad because my relationship isn't going well. Is there anything that could help comfort me?\n",
    "            Agent: It sounds like you’re going through a tough time, and that’s completely valid. Don't forget to care for yourself and seek support from friends or family. It’s okay to give yourself time to work through everything. You’re not alone in this!\n",
    "            Score: 'Perfactly Match'\n",
    "        '''\n",
    "\n",
    "        prompt = ''\n",
    "        with open(f'./evaluator/{agentName}/scenario{scenarioID}.txt', 'r') as file:\n",
    "            prompt += 'Scenario:\\n' + file.read()\n",
    "        \n",
    "        with open(f'./evaluator/{agentName}/{agentName}_eval.txt', 'r') as file:\n",
    "            prompt += 'Agent Introduction:\\n' + file.read()\n",
    "        \n",
    "        dialogue = '\\n'.join(dialogues)\n",
    "        prompt += 'Dialogue:\\n' + dialogue\n",
    "        \n",
    "        message = [\n",
    "            {'role': 'system', 'content': overall_systemPrompt},\n",
    "            {'role': 'assistant', 'content': prompt}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=message,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": OverallEvaluatorResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = json.loads(response.choices[0].message.content)\n",
    "        print(message)\n",
    "        return message['type'], message['reason']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "# not finish yet\n",
    "class Arena():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def oneRound(self, A: str, B: str, scenarioID: int, agentName: str) -> int:\n",
    "        print(f'Scenario {scenarioID}')\n",
    "        with open(f'./evaluator/{agentName}/scenario{scenarioID}.txt', 'r') as file:\n",
    "            print(file.read())\n",
    "        print('- '*40)\n",
    "        print(f'A:\\n{A}\\nB:\\n{B}')\n",
    "        print('- '*40)\n",
    "        print('Please evaluate the two agents based on the following criteria:')\n",
    "        print('1. A is better/n2. B is better/n3. tie/n4. both are bad')\n",
    "        print('- '*40)\n",
    "        result = input('Enter the result: ')\n",
    "        print('-'*80)\n",
    "        return int(result)\n",
    "    \n",
    "    def compareReports(self, dir1: str, dir2: str, agentName:str) -> None:\n",
    "        '''\n",
    "        dir1: path to the first agent reports\n",
    "        dir2: path to the second agent reports\n",
    "        '''\n",
    "        # get all filename from dir1 and dir2\n",
    "        files1 = os.listdir(dir1)\n",
    "        files2 = os.listdir(dir2)\n",
    "        N = min(len(files1), len(files2))\n",
    "        results = []\n",
    "        \n",
    "        for i in range(N):\n",
    "            A = ''\n",
    "            B = ''\n",
    "\n",
    "            if random.randint(1, 100) % 2 == 0:\n",
    "                A = '\\n'.join(history1[i]['dialogues'])\n",
    "                B = '\\n'.join(history2[i]['dialogues'])\n",
    "                result = self.oneRound(A, B, i+1, agentName)\n",
    "                results.append(result)\n",
    "            else:\n",
    "                A = '\\n'.join(history2[i]['dialogues'])\n",
    "                B = '\\n'.join(history1[i]['dialogues'])\n",
    "                result = self.pk(firstDialog, secondDialog, i+1, agentName)\n",
    "                result = 3 - result if result < 3 else result\n",
    "                results.append(result)\n",
    "            \n",
    "        report = {\n",
    "            history1['version']: len([1 for result in results if result == 1]),\n",
    "            history2['version']: len([1 for result in results if result == 2]),\n",
    "            'tie': len([1 for result in results if result == 3]),\n",
    "            'both_bad': len([1 for result in results if result == 4])\n",
    "        }\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 'dialogs1'\n",
    "d2 = 'dialogs2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Good Response but not match', 'reason': 'Sparky does provide an introduction to a specific agent (Bruno) that can help with stress, which is helpful. However, Sparky does not provide a brief introduction to the main features of the app or offer a tour/tutorial for exploring further functionalities, which are part of the expected agent behaviors.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Good Response but not match',\n",
       " 'Sparky does provide an introduction to a specific agent (Bruno) that can help with stress, which is helpful. However, Sparky does not provide a brief introduction to the main features of the app or offer a tour/tutorial for exploring further functionalities, which are part of the expected agent behaviors.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = ['user: What does this app do?', 'agent: This app is here to connect you with friendly animal specialists who can help you with various challenges! Whether it\\'s stress, procrastination, or making friends, we\\'ve got furry and buzzing friends ready to lend a paw or wing!', 'user+ How can you help me?', 'agent: What are you struggling with right now? Whether it\\'s stress, procrastination, or something else, I can introduce you to the perfect animal specialist!', 'user :I\\'m not sure yet, but I would like to find something to help with stress.', 'agent: I have just the friend for you! Meet Bruno, the mindfulness master bear who can help you find calm and tackle stress—would you like to chat with him?', 'user: Yes, I would love to chat with Bruno and learn how to tackle stress!', 'agent: Fantastic! Bruno is waiting for you, ready to share his peaceful paws and calming wisdom. Get ready to relax and unwind!']\n",
    "evaluator = Evaluator(agentName = 'sparky', scenarioID = 1)\n",
    "evaluator.overall_evaluate(dialogue, 'sparky', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Evaluator(AGENT_NAME, SCENARIO_ID)\n",
    "test.evaluate('What does this app do?', 'I do not know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Report():\n",
    "    def __init__(self) -> None:\n",
    "        self.finish = True\n",
    "        self.chatHistory = []\n",
    "\n",
    "    def addHistory(self, chat):\n",
    "        self.chatHistory.append(chat)\n",
    "    \n",
    "    def generateReport(self, filename:str):\n",
    "        df = pd.DataFrame(self.chatHistory)\n",
    "        df.to_csv(f'{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 6000\n",
    "MAX_CHAT = 10\n",
    "SCENARIO_ID = 1\n",
    "PERSONA = 'rebellious_people'\n",
    "AGENT_NAME = 'sparky'\n",
    "PROMPT_VERSION = 'V0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "overall_evaluate = []\n",
    "animal = Animal(name= AGENT_NAME, version= PROMPT_VERSION)\n",
    "\n",
    "for i in trange(1,24):\n",
    "\n",
    "    SCENARIO_ID = i\n",
    "    person = Person(scenarioID = SCENARIO_ID, persona = PERSONA)\n",
    "    # animal = Animal(name= AGENT_NAME)\n",
    "    animal.create_thread()\n",
    "    evaluator = Evaluator(agentName= AGENT_NAME, scenarioID= SCENARIO_ID)\n",
    "    report = Report()\n",
    "\n",
    "    totalToken = 0\n",
    "    totalChat = 0\n",
    "    dialogue = []\n",
    "\n",
    "    while not person.leaveChat:\n",
    "        # chat\n",
    "        personMessage, personInfo = person.say()\n",
    "        animal.listen(personMessage)\n",
    "        animalMessage, animalInfo = animal.say()\n",
    "        person.listen(animalMessage)\n",
    "\n",
    "        # print(f'User: {personMessage}\\nAgent: {animalMessage}')\n",
    "        # print(f'user leave chat: {person.leaveChat}')\n",
    "\n",
    "        # metrics\n",
    "        accuracy, practicality = evaluator.evaluate(personMessage, animalMessage)\n",
    "        # print(f'Accuracy: {accuracy}, Practicality: {practicality}\\n')\n",
    "        \n",
    "        history = {\n",
    "            'person_say': personMessage,\n",
    "            'animal_action': animalInfo['action'],\n",
    "            'animal_say': animalMessage,\n",
    "            'animal_token': animalInfo['token'],\n",
    "            'accuracy': accuracy,\n",
    "            'practicality': practicality,\n",
    "        }\n",
    "        dialogue.append(f'user: {personMessage}, agent: {animalMessage}')\n",
    "\n",
    "        report.addHistory(history)\n",
    "        totalChat += 1\n",
    "        totalToken += animalInfo['token']\n",
    "\n",
    "        if totalToken > MAX_TOKEN or totalChat > MAX_CHAT:\n",
    "            report.finish = False\n",
    "            break\n",
    "\n",
    "    report.generateReport(filename=f'report_{AGENT_NAME}_{SCENARIO_ID}')\n",
    "    \n",
    "    classification = evaluator.overall_evaluate(dialogue, AGENT_NAME, SCENARIO_ID)\n",
    "    overall_evaluate.append(classification)\n",
    "    \n",
    "    print(totalToken, totalChat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{AGENT_NAME}_{PROMPT_VERSION}_overall_evaluate.csv'\n",
    "\n",
    "df = pd.DataFrame(overall_evaluate, columns=['type', 'reason'])\n",
    "df.insert(0, 'scenario_id', range(1, len(df) + 1))\n",
    "\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fauna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
