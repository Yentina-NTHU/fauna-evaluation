{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 6000\n",
    "MAX_CHAT = 10\n",
    "SCENARIO_ID = 1\n",
    "PERSONA = 'emma'\n",
    "AGENT_NAME = 'bizy'\n",
    "PROMPT_VERSION = 'V1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class UserAction(str, Enum):\n",
    "    say = \"say\"\n",
    "    leave = \"leave\"\n",
    "\n",
    "class UserResponse(BaseModel):\n",
    "    action: UserAction\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Person:\n",
    "    def __init__(self, scenarioID: int, persona: str):\n",
    "\n",
    "        scenarioFile = open(f'./user/{AGENT_NAME}/scenario{scenarioID}.txt', 'r')\n",
    "        scenario = scenarioFile.read()\n",
    "        scenarioFile.close()\n",
    "\n",
    "        personaFile = open(f'./persona/{persona}.txt', 'r')\n",
    "        persona = personaFile.read()\n",
    "        personaFile.close()\n",
    "        example = '''\n",
    "                    Example1:\n",
    "                    Input: Are you looking for relaxation techniques or some fun trivia games to de-stress? Let me help you find the best fit!\n",
    "                    Output: {action: say, answer: 'I am looking for relaxation techniques'}\n",
    "                    Example2:\n",
    "                    Input: There's a wonderful bear named Bruno who specializes in relaxation techniques. Would you like to meet him for some calming mindfulness tips?\n",
    "                    Output: {action: leave, answer: 'Yes, meeting Bruno sounds lovely! I would love to get some calming mindfulness tips from him.'}\n",
    "                  '''\n",
    "        systemPrompt = f'You are a user talking to AI APP which can help you deal with your problem during break time. \\\n",
    "                            This is your persona: {persona}\\\n",
    "                            Please play the role according to the scenario: {scenario}\\\n",
    "                            Use Action → Answer structure for responses.\\\n",
    "                            Available Actions:\\\n",
    "                            1. say: respond base on persona and scenario\\\n",
    "                            2. leave: leave the chat when you think the conversation is over, no need to continue\\\n",
    "                            Examples:\\n{example}'\n",
    "        # print(systemPrompt)\n",
    "\n",
    "        self.messages = [\n",
    "            {'role': 'system', 'content': systemPrompt}, \n",
    "        ]\n",
    "        self.leaveChat = False\n",
    "    \n",
    "    def say(self):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=self.messages,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": UserResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message.content\n",
    "        self.messages.append({'role': 'assistant', 'content': message})\n",
    "\n",
    "        # str to dict\n",
    "        message = json.loads(message)\n",
    "        self.leaveChat = (message['action'] == 'leave')\n",
    "\n",
    "        info = {\n",
    "            'token': response.usage.total_tokens,\n",
    "        }\n",
    "        \n",
    "        return message['answer'], info\n",
    "    \n",
    "    def listen(self, message: str):\n",
    "        self.messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tmpAction(str, Enum):\n",
    "    hi = \"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tmpResponseFormat(BaseModel):\n",
    "    action: tmpAction\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Animal:\n",
    "    def __init__(self, name, version: str):\n",
    "\n",
    "        self.agent_name = name\n",
    "        agentFile = open(f'./agent/{name}_{version}.txt', 'r')\n",
    "        agentPrompt = agentFile.read()\n",
    "        agentFile.close()\n",
    "\n",
    "        self.responseFormat = {\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                    \"name\":\"whocares\", \n",
    "                    \"schema\": self.get_response_schema()\n",
    "                }\n",
    "        }\n",
    "        self.systemPrompt = [\n",
    "            {'role': 'system', 'content': agentPrompt}, \n",
    "        ]\n",
    "        self.messages = []\n",
    "    \n",
    "    def get_response_schema(self):\n",
    "        return tmpResponseFormat.model_json_schema()\n",
    "    \n",
    "    def say(self):\n",
    "        \n",
    "        prompt = self.systemPrompt + self.messages\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=prompt,\n",
    "            response_format=self.responseFormat\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message.content\n",
    "        self.messages.append({'role': 'assistant', 'content': message})\n",
    "\n",
    "        message = json.loads(message)\n",
    "        self.leaveChat = (message['action'] == 'leave')\n",
    "\n",
    "        info = {\n",
    "            'token': response.usage.total_tokens,\n",
    "        }\n",
    "        \n",
    "        return message['answer'], info\n",
    "    \n",
    "    def listen(self, message: str):\n",
    "        self.messages.append({'role': 'user', 'content': message})\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparkyActionV0(str, Enum):\n",
    "    call_bruno = \"call_bruno\"\n",
    "    call_bizy = \"call_bizy\"\n",
    "    ask_more = \"ask_more\"\n",
    "    introduce_bruno = \"introduce_bruno\"\n",
    "    introduce_bizy = \"introduce_bizy\"\n",
    "    advise = \"advise\"\n",
    "\n",
    "class sparkyActionV1(str, Enum):\n",
    "    guide_to_bruno = \"guide_to_bruno\"\n",
    "    guide_to_bizy = \"guide_to_bizy\"\n",
    "    explore = \"explore\"\n",
    "    introduce_bruno = \"introduce_bruno\"\n",
    "    introduce_bizy = \"introduce_bizy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparkyResponseFormat(BaseModel):\n",
    "    action: sparkyActionV1\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparky(Animal):\n",
    "\n",
    "    def get_response_schema(self):\n",
    "        return sparkyResponseFormat.model_json_schema()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bizyActionV0(str, Enum):\n",
    "    greet = \"greet\"\n",
    "    start_analysis = \"start_analysis\"\n",
    "    analysing = \"analysing\"\n",
    "    finish_analysis = \"finish_analysis\"\n",
    "    ask_excuse = \"ask_excuse\"\n",
    "    change_excuse = \"change_excuse\"\n",
    "    advise = \"advise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bizy0ActionV1(str, Enum):\n",
    "    greet = \"greet\"\n",
    "    analysis = \"analysis\"\n",
    "    advise = \"advise\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bizyResponseFormat(BaseModel):\n",
    "    action: bizy0ActionV1\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bizy1ActionV1(str, Enum):\n",
    "    start_analysis = \"start_analysis\"\n",
    "    analysing = \"analysing\"\n",
    "    finish_analysis = \"finish_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bizy1ResponseFormat(BaseModel):\n",
    "    action: bizy1ActionV1\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bizy(Animal):\n",
    "    \n",
    "    def get_response_schema(self):\n",
    "        return bizyResponseFormat.model_json_schema()\n",
    "    \n",
    "    def say(self):\n",
    "        prompt = self.systemPrompt + self.messages\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=prompt,\n",
    "            response_format=self.responseFormat\n",
    "        )\n",
    "\n",
    "        message = response.choices[0].message.content\n",
    "        self.messages.append({'role': 'assistant', 'content': message})\n",
    "\n",
    "        message = json.loads(message)\n",
    "\n",
    "        if message['action'] == 'analysis':\n",
    "            with open(f'./agent/bizy1_{PROMPT_VERSION}.txt', 'r') as agentFile:\n",
    "                self.systemPrompt = [\n",
    "                    {'role': 'system', 'content': agentFile.read()}, \n",
    "                ]\n",
    "            self.responseFormat = {\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": bizy1ResponseFormat.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "\n",
    "            #Todo: should say again by bizy1\n",
    "                \n",
    "        if message['action'] == 'finish_analysis':\n",
    "            with open(f'./agent/bizy0_{PROMPT_VERSION}.txt', 'r') as agentFile:\n",
    "                self.systemPrompt = [\n",
    "                    {'role': 'system', 'content': agentFile.read()}, \n",
    "                ]\n",
    "            self.responseFormat = {\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": bizyResponseFormat.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "            #Todo: should add summarize for bizy0\n",
    "\n",
    "        info = {\n",
    "            'token': response.usage.total_tokens,\n",
    "            'action': message['action']\n",
    "        }\n",
    "        \n",
    "        return message['answer'], info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Buzz buzz! Let's see what's behind this delay. Little bee, take over!\",\n",
       " {'token': 339, 'action': 'analysis'})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = \"I haven't start study midterm\"\n",
    "bizy = Bizy('bizy0', PROMPT_VERSION)\n",
    "\n",
    "bizy.listen(dialogue)\n",
    "bizy.say()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bzzzz! What feelings come up when you think about studying for the midterm?',\n",
       " {'token': 426, 'action': 'start_analysis'})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bizy.say()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bzzzz! Do you feel overwhelmed by the amount of material, or is a task like this unappealing?',\n",
       " {'token': 475, 'action': 'analyzing'})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bizy.listen('it\\'s boring')\n",
    "bizy.say()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class responseType(str, Enum):\n",
    "    perfectly_match = \"Perfectly Match\"\n",
    "    good_response = \"Good Response\"\n",
    "    bad_response = \"Bad Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorResponse(BaseModel):\n",
    "    accuracy: int\n",
    "    practicality: int\n",
    "\n",
    "class OverallEvaluatorResponse(BaseModel):\n",
    "    type: responseType\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Evaluator():\n",
    "    def __init__(self, agentName: str, scenarioID: int) -> None:\n",
    "\n",
    "        systemPrompt = '''You are an evaluator. I will provide you with a user’s statement and an agent’s response.\n",
    "                            You should evaluate the accuracy and practicality base on the scenario.\n",
    "                            - Accuracy: Score from 0 to 10. This measures whether the model’s response appropriately addresses the user’s statement.\n",
    "                            - Practicality: Score from 0 to 10. This evaluates whether the model’s suggestion is helpful to the user.\n",
    "                        '''\n",
    "        \n",
    "        with open(f'./evaluator/{agentName}/scenario{scenarioID}.txt', 'r') as file:\n",
    "            systemPrompt += file.read()\n",
    "        with open(f'./evaluator/examples.txt', 'r') as file:\n",
    "            systemPrompt += file.read()\n",
    "    \n",
    "        self.messages = [\n",
    "            {'role': 'system', 'content': systemPrompt}, \n",
    "        ]\n",
    "        \n",
    "    def evaluate(self, personMessage: str, animalMessage: str):\n",
    "        self.messages.append({'role': 'user', 'content': f'User: {personMessage}\\nAgent: {animalMessage}'})\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=self.messages,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": EvaluatorResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "        return message['accuracy'], message['practicality']\n",
    "    \n",
    "    def overall_evaluate(self, dialogues: list, agentName, scenarioID):\n",
    "        overall_systemPrompt = ''' You are an evaluator. Now You have to evaluate agent's behavior.\n",
    "                                I will provide you a scenario with expect agent behaviors and a dialogue contains user's statement and an agent's response.\n",
    "                                User's question and answer might lead to different types of agent responses. So it's important to consider both scenario expect behavior and the whole dialogue before making a decision.\n",
    "                                You should classify the agent's response into one of the following types:\n",
    "                                1. 'Perfectly Match' : Match at least one of the expected agent behaviors we provided.\n",
    "                                2. 'Good Response' : Didn't match any of the expected agent behavior we provided, but still a good response that can help the user.\n",
    "                                3. 'Bad Response' : Didn't match the expected agent behavior we provided, and can not help the user or not practical.\n",
    "\n",
    "                                For Reason, please be concise and brief, reply less than 15 words. \n",
    "                                    Perfectly Match: Match which behavior.\n",
    "                                    Good Response: How it can help the user.\n",
    "                                    Bad Response: Why it can not help the user.\n",
    "                                **Perform action call/guide_to directly without introduce is definetly bad response.**\n",
    "                            '''\n",
    "\n",
    "        prompt = ''\n",
    "        with open(f'./evaluator/{agentName}/scenario{scenarioID}.txt', 'r') as file:\n",
    "            prompt += 'Scenario:\\n' + file.read()\n",
    "        \n",
    "        with open(f'./evaluator/{agentName}/{agentName}_eval.txt', 'r') as file:\n",
    "            prompt += 'Agent Introduction:\\n' + file.read()\n",
    "        \n",
    "        dialogue = '\\n'.join(dialogues)\n",
    "        prompt += 'Dialogue:\\n' + dialogue\n",
    "        \n",
    "        message = [\n",
    "            {'role': 'system', 'content': overall_systemPrompt},\n",
    "            {'role': 'assistant', 'content': prompt}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=message,\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"whocares\", \n",
    "                        \"schema\": OverallEvaluatorResponse.model_json_schema()\n",
    "                    }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message = json.loads(response.choices[0].message.content)\n",
    "        print(message)\n",
    "        return message['type'], message['reason']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Report():\n",
    "    def __init__(self) -> None:\n",
    "        self.finish = True\n",
    "        self.chatHistory = []\n",
    "\n",
    "    def addHistory(self, chat):\n",
    "        self.chatHistory.append(chat)\n",
    "    \n",
    "    def generateReport(self, filename:str):\n",
    "        df = pd.DataFrame(self.chatHistory)\n",
    "        df.to_csv(f'{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:13<01:46, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Analyzed procrastination and offered practical advice.'}\n",
      "2727 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:26<01:34, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Provided tips for creating a study plan effectively.'}\n",
      "3475 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:44<01:32, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Good Response', 'reason': 'Suggesting to break tasks down helps manage overwhelm effectively.'}\n",
      "4378 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:57<01:11, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Provided strategies for enhancing productivity by breaking tasks into smaller steps.'}\n",
      "2799 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [01:18<01:07, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Suggested breaking tasks into smaller steps and celebrating progress.'}\n",
      "4432 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [01:36<00:51, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Bad Response', 'reason': 'No specific suggestions for improving learning environment provided.'}\n",
      "3655 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [01:49<00:31, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Provided methods to increase learning motivation.'}\n",
      "3094 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [02:07<00:16, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Bad Response', 'reason': 'No practical advice given for time management.'}\n",
      "4485 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:21<00:00, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Perfectly Match', 'reason': 'Teaches breaking tasks into smaller parts and using timers effectively.'}\n",
      "3248 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "overall_evaluate = []\n",
    "for i in trange(1,10):\n",
    "\n",
    "    SCENARIO_ID = i\n",
    "    person = Person(scenarioID = SCENARIO_ID, persona = PERSONA)\n",
    "    bizy = Bizy('bizy0', 'V1')\n",
    "    evaluator = Evaluator(agentName= AGENT_NAME, scenarioID= SCENARIO_ID)\n",
    "    report = Report()\n",
    "\n",
    "    totalToken = 0\n",
    "    totalChat = 0\n",
    "    dialogue = ''\n",
    "\n",
    "    while not person.leaveChat:\n",
    "        # chat\n",
    "        personMessage, personInfo = person.say()\n",
    "        bizy.listen(personMessage)\n",
    "        animalMessage, animalInfo = bizy.say()\n",
    "        if(animalInfo['action'] == 'analysis'):\n",
    "            animalMessage, animalInfo = bizy.say()\n",
    "        person.listen(animalMessage)\n",
    "\n",
    "        history = {\n",
    "            'person_say': personMessage,\n",
    "            'animal_action': animalInfo['action'],\n",
    "            'animal_say': animalMessage,\n",
    "            'animal_token': animalInfo['token'],\n",
    "        }\n",
    "        dialogue += f'user: {personMessage}, agent: {animalMessage}'\n",
    "\n",
    "        report.addHistory(history)\n",
    "        totalChat += 1\n",
    "        totalToken += animalInfo['token']\n",
    "\n",
    "        if totalToken > MAX_TOKEN or totalChat > MAX_CHAT:\n",
    "            report.finish = False\n",
    "            break\n",
    "\n",
    "    report.generateReport(filename=f'report_{AGENT_NAME}_{SCENARIO_ID}')\n",
    "    \n",
    "    classification = evaluator.overall_evaluate(dialogue, AGENT_NAME, SCENARIO_ID)\n",
    "    overall_evaluate.append(classification)\n",
    "    \n",
    "    print(totalToken, totalChat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{AGENT_NAME}_{PROMPT_VERSION}_overall_evaluate.csv'\n",
    "\n",
    "df = pd.DataFrame(overall_evaluate, columns=['type', 'reason'])\n",
    "df.insert(0, 'scenario_id', range(1, len(df) + 1))\n",
    "\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fauanTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
